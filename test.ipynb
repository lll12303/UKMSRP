{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-06T17:51:46.585044Z",
     "start_time": "2025-12-06T17:51:44.830801Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "test_full_pipeline.py\n",
    "\n",
    "This script performs a FULL END-TO-END test of the project pipeline using ONLY\n",
    "one input file: synthetic_full_data.csv\n",
    "\n",
    "It tests:\n",
    "    1. Follow-up time calculation\n",
    "    2. Unimodal XGBoost training\n",
    "    3. Unimodal prediction + metrics\n",
    "    4. Cox large-scale regression\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ===== Import project modules =====\n",
    "from src.statistics.compute_followup import compute_followup_pipeline\n",
    "from src.models.train_unimodal import train_unimodal_from_config\n",
    "from src.models.predict_unimodal import main_from_config\n",
    "from src.statistics.cox_analysis import run_large_scale_cox   # you already generated\n",
    "# (If your cox module file name differs, adjust the import)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:51:46.644451Z",
     "start_time": "2025-12-06T17:51:46.604183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 0 — Load the ONLY required file\n",
    "# ================================================================\n",
    "FULL_DATA = \"synthetic_full_data.csv\"\n",
    "\n",
    "assert os.path.exists(FULL_DATA), \\\n",
    "    \"ERROR: synthetic_full_data.csv not found. Please generate it first.\"\n",
    "\n",
    "df_full = pd.read_csv(FULL_DATA)\n",
    "\n",
    "print(\"\\n====== 0. Loaded full dataset ======\")\n",
    "print(df_full.head())"
   ],
   "id": "b0e9ce32d40b4a75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== 0. Loaded full dataset ======\n",
      "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
      "0   0.496714  -0.138264   0.647689   1.523030  -0.234153  -0.234137   \n",
      "1   0.324084  -0.385082  -0.676922   0.611676   1.031000   0.931280   \n",
      "2  -1.415371  -0.420645  -0.342715  -0.802277  -0.161286   0.404051   \n",
      "3   0.250493   0.346448  -0.680025   0.232254   0.293072  -0.714351   \n",
      "4   0.357787   0.560785   1.083051   1.053802  -1.377669  -0.937825   \n",
      "\n",
      "   feature_7  feature_8  feature_9  feature_10  ...  feature_46  feature_47  \\\n",
      "0   1.579213   0.767435  -0.469474    0.542560  ...   -0.719844   -0.460639   \n",
      "1  -0.839218  -0.309212   0.331263    0.975545  ...   -1.463515    0.296120   \n",
      "2   1.886186   0.174578   0.257550   -0.074446  ...    0.781823   -1.236951   \n",
      "3   1.865775   0.473833  -1.191303    0.656554  ...    0.385317   -0.883857   \n",
      "4   0.515035   0.513786   0.515048    3.852731  ...   -0.334501   -0.474945   \n",
      "\n",
      "   feature_48  feature_49  feature_50  Participant ID  status    Region  \\\n",
      "0    1.057122    0.343618   -1.763040          100000       0   England   \n",
      "1    0.261055    0.005113   -0.234587          100001       0     Wales   \n",
      "2   -1.320457    0.521942    0.296985          100002       0   England   \n",
      "3    0.153725    0.058209   -1.142970          100003       0  Scotland   \n",
      "4   -0.653329    1.765454    0.404982          100004       0   England   \n",
      "\n",
      "   baseline time  Date of death  \n",
      "0     2006-04-08            NaN  \n",
      "1     2008-11-01            NaN  \n",
      "2     2008-02-23            NaN  \n",
      "3     2007-08-06            NaN  \n",
      "4     2008-07-15            NaN  \n",
      "\n",
      "[5 rows x 55 columns]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:51:47.578245Z",
     "start_time": "2025-12-06T17:51:47.474068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 1 — Compute Follow-up Time\n",
    "# ================================================================\n",
    "print(\"\\n====== 1. Computing Follow-up ======\")\n",
    "\n",
    "followup = compute_followup_pipeline(\n",
    "    df_full[[\"Participant ID\",\n",
    "             \"Region\",\n",
    "             \"baseline time\",\n",
    "             \"Date of death\",\n",
    "             \"status\"]]\n",
    ")\n",
    "\n",
    "followup.to_csv(\"test_followup.csv\", index=False)\n",
    "print(\"Follow-up saved to test_followup.csv\")\n",
    "print(followup.head())"
   ],
   "id": "1b1893b7ac4d4f43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== 1. Computing Follow-up ======\n",
      "Follow-up saved to test_followup.csv\n",
      "   Participant ID  time  status\n",
      "0          100000  6050       0\n",
      "1          100001  4959       0\n",
      "2          100002  5364       0\n",
      "3          100003  5504       0\n",
      "4          100004  5221       0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:59:42.932165Z",
     "start_time": "2025-12-06T17:59:42.882560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 2 — Prepare merged dataset for XGBoost training\n",
    "# ================================================================\n",
    "print(\"\\n====== 2. Preparing data for XGBoost ======\")\n",
    "\n",
    "df_train = df_full.merge(\n",
    "    followup[[\"Participant ID\", \"status\", \"time\"]],\n",
    "    on=\"Participant ID\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_dup\")\n",
    ")\n",
    "\n",
    "feature_cols = [col for col in df_full.columns\n",
    "                if col.startswith(\"feature_\")]\n",
    "\n",
    "train_input = df_train[[\"Participant ID\"] + feature_cols + [\"status\"]]\n",
    "train_input.to_csv(\"test_unimodal_features.csv\", index=False)\n",
    "\n",
    "print(\"Training file saved as test_unimodal_features.csv\")\n"
   ],
   "id": "9d8b59c049b2244f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== 2. Preparing data for XGBoost ======\n",
      "Training file saved as test_unimodal_features.csv\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:00:04.867449Z",
     "start_time": "2025-12-06T18:00:04.080804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 3 — Train Unimodal XGBoost\n",
    "# ================================================================\n",
    "print(\"\\n====== 3. Training Unimodal XGBoost ======\")\n",
    "\n",
    "train_config = {\n",
    "    \"feature_file\": \"test_unimodal_features.csv\",\n",
    "    \"label_column\": \"status\",\n",
    "    \"index_col\": \"Participant ID\",\n",
    "    \"output_model\": \"test_xgb_model.joblib\",\n",
    "    \"n_folds\": 3\n",
    "}\n",
    "\n",
    "train_unimodal_from_config(train_config)\n",
    "print(\"Model saved: test_xgb_model.joblib\")"
   ],
   "id": "7f5a61a899cbf0da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== 3. Training Unimodal XGBoost ======\n",
      "[Fold 1/3] AUC = 0.6686\n",
      "[Fold 2/3] AUC = 0.6541\n",
      "[Fold 3/3] AUC = 0.6212\n",
      "\n",
      "Mean CV AUC = 0.6479983660130718\n",
      "Model saved => test_xgb_model.joblib\n",
      "Model saved: test_xgb_model.joblib\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:00:19.516472Z",
     "start_time": "2025-12-06T18:00:19.359113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 4 — Predict using Unimodal XGBoost\n",
    "# ================================================================\n",
    "print(\"\\n====== 4. Predicting + Evaluating ======\")\n",
    "\n",
    "predict_config = {\n",
    "    \"feature_file\": \"test_unimodal_features.csv\",\n",
    "    \"model_file\": \"test_xgb_model.joblib\",\n",
    "    \"output_predictions\": \"test_predictions.csv\",\n",
    "    \"label_column\": \"status\",\n",
    "    \"index_col\": \"Participant ID\",\n",
    "    \"save_metrics\": \"test_metrics.json\"\n",
    "}\n",
    "\n",
    "pred_df, metrics = main_from_config(predict_config)\n",
    "\n",
    "print(\"Predictions saved to test_predictions.csv\")\n",
    "print(\"Metrics:\")\n",
    "print(metrics)"
   ],
   "id": "f89c70eed94c68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== 4. Predicting + Evaluating ======\n",
      "Predictions saved => test_predictions.csv\n",
      "Metrics saved => test_metrics.json\n",
      "{'AUC': 0.9139111111111111, 'Average Precision': 0.8245580407033725, 'Accuracy': 0.968}\n",
      "Predictions saved to test_predictions.csv\n",
      "Metrics:\n",
      "{'AUC': 0.9139111111111111, 'Average Precision': 0.8245580407033725, 'Accuracy': 0.968}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:00:47.470183Z",
     "start_time": "2025-12-06T18:00:40.091069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Step 5 — Run Cox large-scale analysis\n",
    "# ================================================================\n",
    "print(\"\\n====== 5. Running Cox Regression ======\")\n",
    "\n",
    "# Cox needs time + status + features\n",
    "df_cox = df_train.copy()\n",
    "\n",
    "cox_results = run_large_scale_cox(\n",
    "    df=df_cox,\n",
    "    time_col=\"time\",\n",
    "    status_col=\"status\",\n",
    "    exclude_cols=[\"Participant ID\"]\n",
    ")\n",
    "\n",
    "cox_results.to_csv(\"test_cox_results.csv\", index=False)\n",
    "\n",
    "print(\"Cox results saved to test_cox_results.csv\")\n",
    "print(cox_results.head())\n",
    "\n",
    "\n",
    "print(\"\\n======= FULL PIPELINE TEST COMPLETED SUCCESSFULLY =======\")"
   ],
   "id": "b820c39b663e5995",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== 5. Running Cox Regression ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cox analysis successfully finished. Time used: 0.00 hours.\n",
      "Cox results saved to test_cox_results.csv\n",
      "      variable        HR  CI_lower  CI_upper       p_value  n_samples  \\\n",
      "50  status_dup  3.159224  2.700789  3.695473  6.866400e-47        500   \n",
      "6    feature_7  0.828995  0.681564  1.008317  6.050978e-02        500   \n",
      "26  feature_27  0.843630  0.691729  1.028889  9.319370e-02        500   \n",
      "47  feature_48  1.176992  0.968656  1.430135  1.010957e-01        500   \n",
      "13  feature_14  0.864262  0.710196  1.051751  1.453166e-01        500   \n",
      "\n",
      "    -log10(p)           FDR significance  \n",
      "50  46.163271  3.501864e-45           **  \n",
      "6    1.218174  9.714066e-01               \n",
      "26   1.030613  9.714066e-01               \n",
      "47   0.995267  9.714066e-01               \n",
      "13   0.837685  9.714066e-01               \n",
      "\n",
      "======= FULL PIPELINE TEST COMPLETED SUCCESSFULLY =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    7.3s\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
